---
layout: post
title:  "Graphs"
date:   2025-01-13 14:56:36 -0300
categories:
---

I was assigned a college project on [graphs](https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)) and decided to share
my implementation with the world since these are very interesting and useful data structures, and, also, because I got an A at
it so I guess it should be good enough to go public and to be the blog's square one. 

Do note though: I am simply sharing my inkling of experience in this blogging venue and I'll certainly [commit
lots of mistakes during it](https://youtube.com/embed/dvKeCcxD3rQ?start=14&end=29). 
Feel free to [reach out](mailto:omarmsqt@gmail.com) if you spot any!

---
<br>
This is the graph we will be discussing:

![Graph](../../../visual_assets/undirected_graph.png)

The image shows an undirected graph, and, as you can see, it is indeed undirected since edges (also known as links) such
as $$ 8 \to 7 $$ are equivalent to $$ 7 \to 8 $$. Furthermore, you may notice that the graph is **weighted** which will be important
in the future as we will be implementing breadth-first search and Kruskal's algorithm.

Still, before we delve into the realm of graph algorithms, we aim to visualize it first and foremost.
In order to achieve this, clever people came up (or discovered if you will) two main ways of *describing* graphs:

- [Adjacency list](https://en.wikipedia.org/wiki/Adjacency_list)
- [Adjacency matrix](https://en.wikipedia.org/wiki/Adjacency_matrix)

This is a very important point regarding this data structure: although graphs are usually pictured 
like the screenshot above i.e with cute, rounded vertices and clear weights in the edges, in 
real life, people work with *representations* of them. I do not know why's that but I am guessing
it's easier and, specially when implemented in a low level language, you get to confront
the **multidimensionality** of graphs.

### Project structure
I coded this in C, partly because it's mandatory as per course specification, although I should mention
that I enjoy coding in this language. The project's structure is available below and can be accessed
by appending `projects/graphs` to my blog's root URL so you can see it all.

I wholeheartedly receive critics, suggestions, and contributions, but since I have zero expectations
regarding people commiting to this project, it's not Git versioned (yet!).

- [graph.c](../../../projects/graphs/graph.c)
- [graph.h](../../../projects/graphs/graph.h)
- [main.c](../../../projects/graphs/main.c)
- [queue.c](../../../projects/graphs/queue.c)
- [queue.h](../../../projects/graphs/queue.h)

The first two `graph.h` and `graph.c` files correspond to definitions and implementations of the graph API which we will
be using in `main.c`, whereas `queue.h` and `queue.c` contain the definitions and implementations
of a simple FIFO [queue](https://en.wikipedia.org/wiki/Queue_(abstract_data_type)).

### Code description
_In this and in the following sections, I'll discuss theory and code. The code snippets
are, for the most part, actual lines of the source files. Still, you may treat them as pseudocode
for the sole fact that important `NULL` checks are stripped for brevity. Nonetheless, the steps
described for the algorithms are actual code._

Let us first understand the declared data types in `graph.h` used to represent the graph: 
{% highlight c %}
{% include_absolute projects/graphs/graph.h, 4-17 %}
{% endhighlight %}


The `Neighbour` struct represents an adjacency list of an arbitrary vertex `i` -- showing
the `vertexValue` of the neighbouring node, the `weight` of its edge and a pointer to the `next` possible neighbouring
node of `i`. You can keep traversing this linked list until you hit the case in which `next` is `NULL`, indicating
that there are no more neighbours of `i` left.

The `Graph` type includes two simple integer counters that track the *maximum* number of vertices `nv` 
-- read-only, passed during initialization -- and the number of edges `ne` that grows as these are included in the DS.
The `Neighbour** nb` has the actual graph representation as a list of adjacency lists: you get the neighbours of
a given vertex `i` by reading `nb[i]` in linear time.

I should say, though, that the `graph.h` was supplied as part of the assignment and, thus, I couldn't really
think of new, more interesting ways of representing the graph, but I think that having $$ O(n) $$ time complexity
here shouldn't be much of an issue. Also, I do not know of further optimizations people do regarding this DS:
[it's a whole part of Computer Science](https://en.wikipedia.org/wiki/Graph_theory).

The last custom type is called `Edge` which I created for use in Kruskal's algorithm:
{% highlight c %}
{% include_absolute projects/graphs/graph.h, 20-24 %}
{% endhighlight %}

The significant difference from this data type to `Neighbour` lies in the fact that `Edge`
encapsulates all the necessary information for describing an edge in a graph inside its memory
layout. The two vertices `u` and `v` along with the weight are all the information needed
to generate a minimum spanning tree. More on that later.

Finally, the ~~public~~ exposed functions from the `graph.c` file are listed below:
{% highlight c %}
{% include_absolute projects/graphs/graph.h, 26-32 %}
{% endhighlight %}

I won't blab about all these functions since some are trivial to understand and quite standard from a C programming perspective.
The focus of this post lies on `bfs`, `kruskal`, and some `static` helper functions declared in the source file which are crucial
for the latter.

Still, before we dive deep into breadth-first search and Kruskal's, I'll unfurl the queue implementation for a matter
of completion. Feel free to skip it, if you fancy.

{% highlight c %}
{% include_absolute projects/graphs/queue.h, 4-17 %}
{% endhighlight %}

The queue will only really be used in BFS. As such, the first type `QueueNode` only needs a pointer
to the neighbouring vertex and another one for the next node in queue.
The `Queue` itself has members for the front and the back of the ADT, since 
we'll be needing constant time for both `front` read and `back` write -- afterall, this where this data
structure excels.

The exposed functions are:
{% highlight c %}
{% include_absolute projects/graphs/queue.h, 20-24 %}
{% endhighlight %}

Again, pretty usual stuff. Both `enqueue` and `dequeue` run in $$O(1)$$ time just like a standard queue.
`is_queue_empty` evaluates the `q->front` pointer giving us information on remaining vertices in queue.
Consequentially, we are able control the graph traversal using this auxiliary structure.

An interesting difference in my `dequeue` implementation is that it encompasses **peeking and popping**, i.e.
it returns the `QueueNode` in front of the queue and moves on with it. I found this approach
easier to implement in the BFS algorithm, even though you have to be extra careful regarding leaking
memory and losing information.

### [Breadth-first search (BFS)](https://en.wikipedia.org/wiki/Breadth-first_search)

The BFS implementation starts with the creation of two auxiliary structures necessary for the graph traversal logic.
A `verticesToVisit` queue which holds enqueued vertices during the traversal for the later exploration
of their neighbours. 

{% highlight c %}
{% include_absolute projects/graphs/graph.c, 120-127 %}
{% include_absolute projects/graphs/graph.c, 132-132 %}
{% endhighlight %}

Notice that the `visited` array is a pointer of `int` and is allocated using `calloc`. Unlike the college canon, `malloc`,
this API allocates a block of memory and [zero initializes all of its bytes to zero](https://en.cppreference.com/w/c/memory/calloc).
Due to this behavior, I could leverage the initial status of zero (false) in the `visited` array as a way of saying that all
vertices were initially unvisited, which makes sense.

At the end of the day, this array is just a pointer to boolean states, so you may query
`visited[i]` of a given vertex `i` and check its result: $$1$$ in case it has been visited, $$0$$ otherwise.
All in all, it behaves just like the `g->nb` list of adjacency lists of the graph.

The gist of the algorithm lies in a `do {...} while()` loop. The old implementation I made used a "usual" `while` loop, but it 
required an auxiliary, heap allocated `initialVertex` variable to be enqueued and, thus, allow the loop to start.
Once again, I do not know if I made the right decision -- if there's such thing --, but I refactored the algorithm to 
make use of the former C construct.

{% highlight c %}
{% include_absolute projects/graphs/graph.c, 139-142 %}
{% include_absolute projects/graphs/graph.c, 144-145 %}
{% include_absolute projects/graphs/graph.c, 152-153 %}
{% include_absolute projects/graphs/graph.c, 161-161 %}
{% include_absolute projects/graphs/graph.c, 188-188 %}
{% endhighlight %}



I found this interesting because such construct immediately enters the loop's body and *later* evaluates the condition,
so it's like the semantics of it express that the **initial run will be different** (which it is). It also discarded
the use of the previously mentioned `initialVertex` heap variable and hacky first-time enqueuing.

The refactoring led to some branching in the loop's internal (to detect if it's a first run), but I don't think it negatively impacted
readability. I know `do {...} while()` loops aren't new in programming, but I rarely used them. Using them in this context
exposed me a to a **new way of thinking about and solving the problem**, which is part of 
my [ethos](https://en.wikipedia.org/wiki/Ethos), so hooray I guess!

Dequeuing from an empty queue will return a `NULL` pointer, which means the program is faced with the initial case
of the traversal. The variable `currentVertexValue` is assigned to `initialVertexValue` (the vertex chosen by the user to start BFS from), and
then used to obtain the adajcency list of it (`currentVertexNeighbours`).


{% highlight c %}
{% include_absolute projects/graphs/graph.c, 144-151 %}
        }
{% endhighlight %}

In order for the algorithm to properly work, the initial vertex has to be immediately marked as visited, otherwise
when it's found during traversal it will be revisited leading to nodes being visited in unexpected order and 
increased runtime. The subsequent vertices will be visited during the adjacency list traversal.

In the `else` branch, the `currentVertex` is firstly extracted from the `qn` queue node, and `currentVertexValue`
is obtained by reading the struct pointer. Analogous to the first case, the adjacency list is obtained through
the global graph struct: `g->nb[currentVertexValue]`

{% highlight c %}
{% include_absolute projects/graphs/graph.c, 152-161 %}
{% endhighlight %}

After the branching is done, the actual traversal in the vertice's adjacency list begins:
{% highlight c %}
{% include_absolute projects/graphs/graph.c, 163-174 %}
                // allocation check and pointer assignment...
{% include_absolute projects/graphs/graph.c, 181-186 %}
{% endhighlight %}

This is pretty much like a linked list traversal so it should run in $$O(E)$$ where $$E$$ is the number of edges in the list.
There's also a copying step along with the `visited` array so the implementation should have $$O(V)$$ space complexity in average 
(where $$V$$ is the amount of vertices in the graph). In spite of this, the time complexity of the algorithm is $$O(V+E)$$ since all 
vertices are visited and all edges are explored.

### [Kruskal's algorithm](https://en.wikipedia.org/wiki/Kruskal%27s_algorithm)
The desired outcome from running Kruskal's is to have a [minimum spanning tree (MST)](https://en.wikipedia.org/wiki/Minimum_spanning_tree)
at the end of the algorithm. In essence, **a MST is a subgraph of an undirected and weighted graph 
that connects all vertices without forming any cycles and has the minimum possible total edge weight**.

As an example, MSTs can be used in network design for establishing a [topology](https://en.wikipedia.org/wiki/Network_topology) 
which ensures all elements (vertices) are connected with the least piping cost (minimum total edge weight)!
How's that for engineering the Internet?

<p align="center">
  <img src="../../../visual_assets/internet_map.jpg" alt="Partial map of 2005 Internet" />
  <br>
  <em>Partial map of Internet in 2005 by the <a href="https://www.opte.org/the-internet">OPTE project</a></em>
  <br>
  <em>Image extracted from Wikipedia and subject to <a href="https://creativecommons.org/licenses/by/2.5/deed.en">Creative Commons</a></em>
</p>

---
<br>
Kruskal is a [greedy algorithm](https://en.wikipedia.org/wiki/Greedy_algorithm), and as such, works heuristically by picking 
the *immediate best local* solution. What that means is that, in our case, the graph edges are sorted by ascending weights,
and then, the ones with the minimal weight that connect two vertices and do not form a cycle are immediately added to the tree,
without ever considering if, in the grand scheme of things, this will be global optimal solution.

Given the study object of the assignment -- a relatively sparse graph -- the locally best solution should be
good enough compared to the global one. Afterall, there are only $$14$$ edges, so the approximation
should be a fair trade.   

In case my explanation on greedy algos was too fleeting, I'll leave a helpful analogy that I came across in the
article on [40 Key Computer Science Concepts Explained In Layman’s Terms](https://carlcheo.com/compsci):

> Imagine you are going for hiking and your goal is to reach the highest peak possible. You already have the map before you start, but there are thousands of possible paths shown on the map. You are too lazy and simply don’t have the time to evaluate each of them. Screw the map! You started hiking with a simple strategy – be greedy and short-sighted. Just take paths that slope upwards the most.
>
> After the trip ended and your whole body is sore and tired, you look at the hiking map for the first time. Oh my god! There’s a muddy river that I should’ve crossed, instead of keep walking upwards.
>
> A greedy algorithm picks the best immediate choice and never reconsiders its choices.

Past the theoretical discussion, let us get our hands dirty in the implementation:
{% highlight c %}
{% include_absolute projects/graphs/graph.c, 195-203 %}
    .......
}
{% endhighlight %}

We `malloc` a block of memory for **all** inserted edges in the graph `g->ne`. This isn't great since
there's a chance that not each and every edge will be analyzed. I'll explain that in detail later, but for now, 
bear with me and let us assume we will make use of all edges, so we allocate memory for all of them.

Next, we traverse entirety of the graph in its $$V$$ vertices and $$E$$ edges using the adjacency lists. So this nested `for`
loop runs in $$O(V+E)$$ linear time. 
{% highlight c %}
{% include_absolute projects/graphs/graph.c, 209-217 %}
{% endhighlight %}

An `if` clause stores and [deduplicates](https://en.wikipedia.org/wiki/Data_deduplication) edges since we're dealing
with an undirected graph. The conditional only stores edges $$ (u, v) $$ where $$ u < v $$. This is a matter of design, since
picking the edge when $$u$$ is greater will also achieve deduplication, but I think that selecting the edges in ascending order
is somehow more intuitive.

Once the edges are collected, they have to be sorted. I used the [C library `qsort`](https://en.cppreference.com/w/c/algorithm/qsort), which,
despite its name is not enforced nor guaranteed by the C standard to be [quicksort](https://en.wikipedia.org/wiki/Quicksort).
{% highlight c %}
{% include_absolute projects/graphs/graph.c, 219-220 %}
{% endhighlight %}

At this point, we face one of the `static` helper functions I mentioned earlier:
{% highlight c %}
{% include_absolute projects/graphs/graph.c, 331-338 %}
{% endhighlight %}

The types and return branches are determined by `qsort`'s API. I just made the function comply to it.

Up next, two crucial heap allocated structures are created. The first one is called `parents` and is responsible
for creating a forest consisting of single node trees for every vertex in the graph.
{% highlight c %}
{% include_absolute projects/graphs/graph.c, 222-224 %}
{% include_absolute projects/graphs/graph.c, 233-236 %}
{% endhighlight %}

The `rank` array is a heuristic approach to each tree's height. Since it's `calloc` allocated, initially all
trees have rank $$0$$. It will be used for updating the forest as we start connecting vertices in Kruskal's
main loop. These two variables make up a [Union-Find data structure](https://en.wikipedia.org/wiki/Disjoint-set_data_structure).

Finally, we get to the core of the matter: the `for` loop.

{% highlight c %}
{% include_absolute projects/graphs/graph.c, 238-239 %}
{% include_absolute projects/graphs/graph.c, 248-254 %}
{% endhighlight %}

Notice how the minimum spanning tree is of type `Graph*`. This not only makes the implementation simpler by
reducing the quantity of custom types, but makes perfect sense since **trees are special cases of graphs**.

The block comment above the loop should make the condition checking logic clear, but I'll add a note
on the loop initialized variables. In the one hand, the variable `i` 
indexes the `edges` array, providing access to the struct pointers `u`, `v`, and `weight`. It is also
used for bounds checking -- `i < g->ne` -- preventing invalid memory access, and for 
halting the algorithm after all edges are checked.

On the other hand, the `includedEdges` variable keeps track of inserted edges and stops the algorithm once it's equal to
`g->nv - 1`: [the spanning tree of a given graph with $$V$$ vertices is complete once it has $$V-1$$ edges](https://en.wikipedia.org/wiki/Minimum_spanning_tree#Possible_multiplicity).
