---
layout: post
title:  "Graphs"
date:   2025-01-13 14:56:36 -0300
categories:
---

I was assigned a college project on [graphs](https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)) and decided to share
my implementation with the world since these are very interesting and useful data structures. Also, I got a great score at
it so I guess it should be good enough to go public and to be the blog's square one. 

---
<br>
Below is a picture of the graph in my assignment:

<div id="discussed-graph">
  <p>
    <img src="../../../visual_assets/undirected_graph.png" alt="Image of the graph of my assignment.
    We will talk about it." />
  </p>
</div>

The image shows a graph that is both **undirected** -- since edges (also known as links) such
as $$ 8 \to 7 $$ are equivalent to $$ 7 \to 8 $$ -- and **weighted**, as each edge is assigned 
a value that typically represents the cost, distance, or strength of the connection between two vertices (or nodes). 

Although these data structures are usually pictured like the screenshot above i.e. with clear, 
rounded vertices and weights, in real life, people work with *representations* of graphs.
To achieve this, two main ways of *describing* them were developed:

1. [Adjacency list](https://en.wikipedia.org/wiki/Adjacency_list)
2. [Adjacency matrix](https://en.wikipedia.org/wiki/Adjacency_matrix)

Both of these representations will be explored in code, but, for this article,
I'll treat the graph as an adjacency list. 

<h3 id="project-structure">
  <a href="#project-structure" style="text-decoration: none; color: blue;">Project structure</a>
</h3>
The project was written in C per course specification. Still, I enjoy coding in this language
as the generated machine code is blazingly fast and allows you to tinker with the computer's memory
to make some custom optimizations. Of course, the language itself isn't a guarantee for
performatic software: usual software development guidelines apply such as using suitable
data structures, taking into account algorithmic complexity of loops, etc.

Programming in C is a double edged sword as you get this amazing advantages, you are also 
solely resposible for memory management, bounds checking, and even being aware of [demons will coming out of your nose](https://en.wikipedia.org/wiki/Undefined_behavior) (this is by far the most dangerous part, as it makes debugging awfully difficult).


You can check out the code in my [GitHub repository](https://github.com/OmarMesqq/graphs), but I'll try
to briefly explain its key files.

<h3 id="code-description">
  <a href="#code-description" style="text-decoration: none; color: blue;">Code description</a>
</h3>
_In this and in the following sections, I'll discuss theory and code. You may treat the code snippets 
as pseudocode for the sole fact that important `NULL` checks are stripped for brevity. 
However, the described steps in algorithm sections represent actual procedures in these routines._

The program's entrypoint, `main.c`, contains the actual usage and [tests](https://en.wikipedia.org/wiki/Test-driven_development)
of the API. The API itself is declared and defined in `graph.h` and `graph.c` respectively. There's also a
"private" (in the C lingo, one may call it `static`) API that is [hidden](https://en.wikipedia.org/wiki/Information_hiding) from the main file, and
contains a [queue](https://en.wikipedia.org/wiki/Queue_(abstract_data_type)) ADT used for graph traversal.

In `graph.h`, some data types are created to describe the graph: 
{% highlight c %}
{% include_absolute https://raw.githubusercontent.com/OmarMesqq/graphs/refs/heads/main/graph.h, 4-14 %}
{% endhighlight %}

The `Neighbour` struct represents an adjacency list of an arbitrary vertex `i` -- showing
the `vertexValue` of the neighbouring node, the `weight` of its edge and a pointer to the `next` possible neighbouring
node of `i`. You can keep traversing this linked list until you hit the case in which `next` is `NULL`, indicating
that there are no more neighbours of `i` left.

The `Graph` type includes two simple integer counters that track the *maximum* number of vertices `nv` 
-- read-only, passed during initialization -- and the number of edges `ne` that grows as these are included in the data structure.
The `Neighbour** nb` field that actually represents the graph as **a list of adjacency lists**: you get the neighbours of
a given vertex `i` by reading `nb[i]` in linear time.

I should say, though, that the `graph.h` was supplied as part of the assignment and, thus, I couldn't really
think of new, interesting ways of representing the graph, but I think that having $$ O(n) $$ time complexity
here shouldn't be much of an issue. Also, I do not know of further optimizations people do regarding graphs:
[it's a whole part of Computer Science](https://en.wikipedia.org/wiki/Graph_theory).

The last custom type is called `Edge` which I created for use in Kruskal's algorithm:
{% highlight c %}
{% include_absolute https://raw.githubusercontent.com/OmarMesqq/graphs/refs/heads/main/graph.h, 16-20 %}
{% endhighlight %}

The `Edge` data type houses all the necessary information for describing an edge of a graph inside its memory
layout. The two vertices `u` and `v` along with the `weight` of the edge between them are all the information needed
to generate a minimum spanning tree. More on that later.

The functions exposed by `graph.h` are listed below:
{% highlight c %}
{% include_absolute https://raw.githubusercontent.com/OmarMesqq/graphs/refs/heads/main/graph.h, 22-28 %}
{% endhighlight %}

I won't blab about all these functions since some are trivial to understand and quite standard from a C programming perspective.
This post focuses on `bfs`, `kruskal`, and some `static` helper functions in the definition file which are crucial
for the latter.

Even though it may be considered a simple topic for technical readers, 
I'll unfurl the queue implementation for a matter of completion. 
Feel free to <a href="#breadth-first-search" style="text-decoration: none; color: blue;">skip it</a>, if you fancy.

In the header file, two custom types are created:
{% highlight c %}
{% include_absolute https://raw.githubusercontent.com/OmarMesqq/graphs/refs/heads/main/queue.h, 5-13 %}
{% endhighlight %}

The queue will only really be used in the breadth-first search algorithm. As such, the `QueueNode` type only needs a pointer
to the adjacency list of the current vertex (`nb`) and another one for the `next` node in queue.
The `Queue` itself has members for the front and the back of the ADT, since 
constant time will be needed for both `front` read and `back` write -- after all, this where this data
structure excels.

The exposed functions are:
{% highlight c %}
{% include_absolute https://raw.githubusercontent.com/OmarMesqq/graphs/refs/heads/main/queue.h, 16-20 %}
{% endhighlight %}

Pretty usual stuff. Both `enqueue` and `dequeue` run in $$O(1)$$ time just like a standard queue.
`is_queue_empty` evaluates the `q->front` pointer giving us information on remaining vertices in queue.
Consequently, it can -- and will -- be used to control the graph traversal.

A point that's worth mentioning is that my `dequeue` implementation encompasses both **peeking and popping**, i.e.
it returns the `QueueNode` in front of the queue and moves on with it. I found this approach
easier to implement in the BFS algorithm, even though you have to be extra careful regarding leaking
memory and losing information.


<h3 id="breadth-first-search">
  <a href="#breadth-first-search" style="text-decoration: none; color: blue;">Breadth-first search (BFS)</a>
</h3>
The [BFS](https://en.wikipedia.org/wiki/Breadth-first_search) implementation (`bfs` in `graph.c`)
starts with the creation of two auxiliary structures necessary for the graph traversal logic.
A `verticesToVisit` queue which holds enqueued vertices during the traversal for the later exploration
of their neighbours. 

{% highlight c %}
{% include_absolute https://raw.githubusercontent.com/OmarMesqq/graphs/refs/heads/main/graph.c, 120-127 %}
{% include_absolute https://raw.githubusercontent.com/OmarMesqq/graphs/refs/heads/main/graph.c, 132-132 %}
{% endhighlight %}

The `visited` array is a pointer of `int` and is allocated using `calloc`. Unlike `malloc`, the college canon,
this function allocates a block of memory and [zero initializes all of its bytes to zero](https://en.cppreference.com/w/c/memory/calloc).
Due to this behavior, I could leverage the initial status of zero (false) in the `visited` array as a way of saying that all
vertices were initially unvisited, which makes sense.

At the end of the day, this array is just a pointer to boolean states, so you may query
`visited[i]` of a given vertex `i` and check its result: $$1$$ in case it has been visited, $$0$$ otherwise.
All in all, it behaves similarly to the graph's `nb` list of adjacency lists.

The gist of the BFS algorithm lies in a `do {...} while()` loop. The old implementation I made used a `while` loop, but it 
required an auxiliary, heap allocated variable to be enqueued, making the checked condition evaluate to true, and, thus, allowing the loop to start.
I do not know if I made the right decision -- if there's such thing --, but I refactored the algorithm to 
leverage the former C construct.

{% highlight c %}
{% include_absolute https://raw.githubusercontent.com/OmarMesqq/graphs/refs/heads/main/graph.c, 139-142 %}
{% include_absolute https://raw.githubusercontent.com/OmarMesqq/graphs/refs/heads/main/graph.c, 144-145 %}
{% include_absolute https://raw.githubusercontent.com/OmarMesqq/graphs/refs/heads/main/graph.c, 152-153 %}
{% include_absolute https://raw.githubusercontent.com/OmarMesqq/graphs/refs/heads/main/graph.c, 161-161 %}
{% include_absolute https://raw.githubusercontent.com/OmarMesqq/graphs/refs/heads/main/graph.c, 188-188 %}
{% endhighlight %}

It was an interesting refactor because a `do {...} while()` *enters the loop's body immediately and evaluates its condition afterwards*,
so it's like the semantics of it express that the **initial run will be different** (which it is). It also discarded
the use of the previously mentioned helper variable and hacky first-time enqueuing.

The refactoring did lead to some branching in the loop's internal (to check if it's a first run), but I don't think it negatively impacted
readability. I know `do {...} while()` loops aren't new in programming, but I rarely used them. Frankly, though, using them in this context
exposed me to a **new way of thinking about and solving the problem**, which is an integral part of the engineering 
[ethos](https://en.wikipedia.org/wiki/Ethos) of continuous improvement.

---
<br>
Dequeuing from an empty queue will return a `NULL` pointer, which means the program is faced with the initial case
of the traversal (first run of the algorithm). 
The variable `currentVertexValue` is assigned to `initialVertexValue` (vertex chosen by the user to start BFS from), and
then used to obtain the adjacency list of it (`currentVertexNeighbours`).


{% highlight c %}
{% include_absolute https://raw.githubusercontent.com/OmarMesqq/graphs/refs/heads/main/graph.c, 144-151 %}
        }
{% endhighlight %}

In order for the algorithm to properly work, the initial vertex has to be immediately marked as visited in this branch, otherwise
when it's found during traversal it will be revisited leading to nodes being visited in unexpected order and 
increased runtime.

The `else` branch will run for subsequent vertices.
Here, the dequeued `qn` node won't be `NULL` and the `currentVertex` is extracted from it. Then, `currentVertexValue`
and `currentVertexNeighbours` are obtained by reading the corresponding struct pointers.

{% highlight c %}
{% include_absolute https://raw.githubusercontent.com/OmarMesqq/graphs/refs/heads/main/graph.c, 152-161 %}
{% endhighlight %}

After the branching is done, the actual traversal in the vertice's adjacency list begins:
{% highlight c %}
{% include_absolute https://raw.githubusercontent.com/OmarMesqq/graphs/refs/heads/main/graph.c, 163-174 %}
                // allocation check and pointer assignment...
{% include_absolute https://raw.githubusercontent.com/OmarMesqq/graphs/refs/heads/main/graph.c, 182-186 %}
{% endhighlight %}

This is pretty much like a linked list traversal so it should run in $$O(E)$$ where $$E$$ is the number of edges in the list.
There's also a copying step along with the `visited` array so the implementation should have $$O(V)$$ space complexity in average 
(where $$V$$ is the amount of vertices in the graph). In spite of this, the time complexity of the algorithm is $$O(V+E)$$ since all 
vertices are visited and all edges are explored.

Starting from vertex **0**, the order in which the vertices of the [analyzed graph](#discussed-graph) are visited is shown below:
```
Visiting 0
Visiting 8
Visiting 1
Visiting 7
Visiting 9
Visiting 3
Visiting 6
Visiting 4
Visiting 5
```

<h3 id="kruskals-algorithm">
  <a href="#kruskals-algorithm" style="text-decoration: none; color: blue;">Kruskal's algorithm</a>
</h3>
The desired outcome from running [Kruskal's](https://en.wikipedia.org/wiki/Kruskal%27s_algorithm)
(`kruskal` in `graph.c`) is to have a 
[minimum spanning tree (MST)](https://en.wikipedia.org/wiki/Minimum_spanning_tree)
at the end of the algorithm. In essence, **a MST is a subgraph of an undirected and weighted graph 
that connects all vertices without forming any cycles and has the minimum possible total edge weight**.

As an example, MSTs can be used in network design for establishing a [topology](https://en.wikipedia.org/wiki/Network_topology) 
which ensures all elements (vertices) are connected with the least piping cost (minimum total edge weight)!
How's that for engineering the Internet?

<p align="center">
  <img src="../../../visual_assets/internet_map.jpg" alt="Partial map of 2005 Internet" />
  <br>
  <em>Partial map of Internet in 2005 by the <a href="https://www.opte.org/the-internet">OPTE project</a></em>
  <br>
  <em>Image extracted from Wikipedia and subject to <a href="https://creativecommons.org/licenses/by/2.5/deed.en">Creative Commons</a></em>
</p>

---
<br>
Kruskal is a [greedy algorithm](https://en.wikipedia.org/wiki/Greedy_algorithm), and as such, works heuristically by picking 
the *immediate best local* solution. What that means is that, for this particular case, the graph edges will sorted by ascending weights,
and then, the ones with the minimal weight that connect two vertices and do not form a cycle are immediately added to the tree,
*without ever considering if, in the grand scheme of things, this arrangement will be global optimal solution*.

Given the relatively sparse graph in this assignment, the locally optimal solution should be sufficiently close to the global one. 
After all, there are only $$14$$ edges, so the approximation, if any, should be a fair trade.   

In case my explanation of greedy algorithms was too fleeting, I'll leave a helpful analogy that I came across in the
article [40 Key Computer Science Concepts Explained In Layman’s Terms](https://carlcheo.com/compsci):

> Imagine you are going for hiking and your goal is to reach the highest peak possible. You already have the map before you start, but there are thousands of possible paths shown on the map. You are too lazy and simply don’t have the time to evaluate each of them. Screw the map! You started hiking with a simple strategy – be greedy and short-sighted. Just take paths that slope upwards the most.
>
> After the trip ended and your whole body is sore and tired, you look at the hiking map for the first time. Oh my god! There’s a muddy river that I should’ve crossed, instead of keep walking upwards.
>
> A greedy algorithm picks the best immediate choice and never reconsiders its choices.

Past the theoretical discussion, let us get our hands dirty in the implementation:
{% highlight c %}
{% include_absolute https://raw.githubusercontent.com/OmarMesqq/graphs/refs/heads/main/graph.c, 195-203 %}
    .......
}
{% endhighlight %}

A block of memory is allocated for **all** inserted edges in the graph. This isn't great since
there's a chance that not each and every edge will be analyzed. I'll explain that in detail later, but for now, 
let's assume all of them will be used.

Next, the entirety of the graph is traversed in its $$V$$ vertices and $$E$$ edges using the adjacency lists. So this nested `for`
loop runs in $$O(V+E)$$ linear time. 
{% highlight c %}
{% include_absolute https://raw.githubusercontent.com/OmarMesqq/graphs/refs/heads/main/graph.c, 209-217 %}
{% endhighlight %}

An `if` clause is responsible for storing and [deduplicating](https://en.wikipedia.org/wiki/Data_deduplication) edges as the
graph is undirected. The conditional only stores edges $$ (u, v) $$ where $$ u < v $$. This is a design matter, since
picking the edge when $$u$$ is greater will also achieve deduplication, but I think that selecting the edges in ascending order
is somehow more intuitive.

Once the edges are collected, they have to be sorted. I used the [C library `qsort`](https://en.cppreference.com/w/c/algorithm/qsort), which,
despite its name is not enforced nor guaranteed by the C standard to be [quicksort](https://en.wikipedia.org/wiki/Quicksort).
{% highlight c %}
{% include_absolute https://raw.githubusercontent.com/OmarMesqq/graphs/refs/heads/main/graph.c, 219-220 %}
{% endhighlight %}

At this point, one of the `static` helper functions I mentioned earlier shows itself:
{% highlight c %}
{% include_absolute https://raw.githubusercontent.com/OmarMesqq/graphs/refs/heads/main/graph.c, 331-338 %}
{% endhighlight %}

The argument types and return branches are determined by `qsort`'s API. I just made the function comply to it.

Up next, two crucial heap allocated structures are created. The first one is called `parents` and is responsible
for creating a forest consisting of single node trees for every vertex in the graph.
{% highlight c %}
{% include_absolute https://raw.githubusercontent.com/OmarMesqq/graphs/refs/heads/main/graph.c, 222-224 %}
{% include_absolute https://raw.githubusercontent.com/OmarMesqq/graphs/refs/heads/main/graph.c, 233-236 %}
{% endhighlight %}

The `rank` array is a heuristic approach to each tree's height. Since it's `calloc` allocated, all
trees initially have rank $$0$$. It will be used for updating the forest as the vertices are connected in Kruskal's
main loop. These two variables make up a [Union-Find data structure](https://en.wikipedia.org/wiki/Disjoint-set_data_structure).

Finally, we get to the core of the matter: the `for` loop.

{% highlight c %}
{% include_absolute https://raw.githubusercontent.com/OmarMesqq/graphs/refs/heads/main/graph.c, 238-239 %}
{% include_absolute https://raw.githubusercontent.com/OmarMesqq/graphs/refs/heads/main/graph.c, 248-254 %}
{% endhighlight %}

Notice how the minimum spanning tree is of type `Graph*`. This not only makes the implementation simpler by
reducing the quantity of custom types, but makes perfect sense since **trees are special cases of graphs**.

The block comment above the loop should make the condition checking logic clear, but I'll add a note
on the loop initialized variables. In the one hand, the variable `i` 
indexes the `edges` array, providing access to the struct pointers `u`, `v`, and `weight`. It is also
used for bounds checking (`i < g->ne`) preventing invalid memory access, and for 
halting the algorithm after all edges are checked.

On the other hand, the `includedEdges` variable keeps track of inserted edges and stops the algorithm once it's equal to
`g->nv - 1`: [the spanning tree of a given graph with $$V$$ vertices is complete once it has $$V-1$$ edges](https://en.wikipedia.org/wiki/Minimum_spanning_tree#Possible_multiplicity).

Inside the loop, all information regarding the currently analyzed edge `i` is accessed:
{% highlight c %}
{% include_absolute https://raw.githubusercontent.com/OmarMesqq/graphs/refs/heads/main/graph.c, 255-257 %}
{% endhighlight %}

For the next step, a conditional statement safeguards the crux of Kruskal's: adding the edge of two vertices 
without forming cycles in the tree.
{% highlight c %}
{% include_absolute https://raw.githubusercontent.com/OmarMesqq/graphs/refs/heads/main/graph.c, 259-264 %}
{% endhighlight %}

If the vertex already is the root of the tree (parent of the set) containing itself, the `_find` function returns
its value immediately. This scenario should be more common at the start of the routine, when each vertex is in its 
own tree.

{% highlight c %}
{% include_absolute https://raw.githubusercontent.com/OmarMesqq/graphs/refs/heads/main/graph.c, 340-349 %}
{% endhighlight %}

Otherwise, it leverages recursion to find the parent of a given vertex already included in the incipient MST.
Here, something amazing happens: until the base case is reached, all the vertices deep in recursion will
have their parent's value assigned to the value returned by the function calls.

This optimization is called [path compression](https://cp-algorithms.com/data_structures/disjoint_set_union.html#path-compression-optimization)
and makes paths to the set's parent shorter, quickening subsequent `_find` calls.
Human ingenuity is amazing!

<p align="center">
  <img src="../../../visual_assets/path_compression.png" alt="Image showing two trees. The one on the left is unoptimized, and 
  the one on the right is flattened due to path compression." />
  <em>Image extracted from <a href="https://cp-algorithms.com/">Algorithms for Competitive Programming</a>.</em>
  <br>
  <em>Notice how the tree on the right is flatter.</em>
</p>

<br>
After `_find` is called for the first time, checking whether two vertices $$u$$ and $$v$$ are in the same set
should be faster as the set's representative will probably be read in $$O(1)$$. Unless the graph is too dense,
the entirety of nodes won't be traversed for the parent check.

In case $$u$$ and $$v$$ are in NOT in the same set, the conditional evaluates to true:

{% highlight c %}
{% include_absolute https://raw.githubusercontent.com/OmarMesqq/graphs/refs/heads/main/graph.c, 264-264 %}
{% endhighlight %}

and they can be included in the MST:

{% highlight c %}
{% include_absolute https://raw.githubusercontent.com/OmarMesqq/graphs/refs/heads/main/graph.c, 265-265 %}
{% endhighlight %}

As the vertices are now in the same set (the MST), the Union Find structure has to be updated:
{% highlight c %}
{% include_absolute https://raw.githubusercontent.com/OmarMesqq/graphs/refs/heads/main/graph.c, 266-268 %}
{% endhighlight %}

The `_unionSets` function does $$u \cup v$$ taking the ranks of both trees in account:

{% highlight c %}
{% include_absolute https://raw.githubusercontent.com/OmarMesqq/graphs/refs/heads/main/graph.c, 351-367 %}
{% endhighlight %}

Inside the [defensive](https://en.wikipedia.org/wiki/Defensive_programming) if clause lies the mechanism
by which two sets are joined. The set with the greatest rank -- recall that it's a heuristic for a tree's height --
is selected as the root of the new $$u \cup v$$ set. **This ensures that the tree doesn't grow unnecessarily since
the smaller tree's rank won't affect the larger one's**.

In case both sets have the same rank, the first one is arbitrarily picked, $$u$$, as the representative of the new
set and increment its rank by one.

Sorting dominates the runtime of Kruskal's algorithm so the main loop shouldn't affect
performance as much as the former step.

Once the algorithmic procedure is done, auxiliary data structures are freed and the MST is returned to the caller:
{% highlight c %}
{% include_absolute https://raw.githubusercontent.com/OmarMesqq/graphs/refs/heads/main/graph.c, 272-276 %}
{% endhighlight %}


<h3 id="usage-example">
  <a href="#usage-example" style="text-decoration: none; color: blue;">Usage example</a>
</h3>

Now that some properties and algorithms of graphs were discussed, I'll put our discussion in practice
by analyzing `main.c`. I made this assignment from a top down perspective,
so I'll first list what the binary is expected to achieve, and later show its source.

1. Create the graph itself (as we've seen, a representation of it)
2. Add the edges that make up the [graph in the assignment](#discussed-graph)
3. Pretty print the graph to `stdout` as an adjacency list and as an adjacency matrix
4. Print the visited vertices in BFS order
5. Using Kruskal's algorithm, obtain a minimum spanning tree (MST)
6. Pretty print the MST to `stdout` as an adjacency list and as an adjacency matrix
7. Correctly manage memory while doing all of that (so no segfaults, leaks, etc)


Most of them are clearly addressed in the binaries entrypoint (`main.c`):
{% highlight c %}
{% include_absolute https://raw.githubusercontent.com/OmarMesqq/graphs/refs/heads/main/main.c, 1-55 %}
{% endhighlight %}

Since all the $$14$$ edges **have** to be inserted, I used an `assert` call there.
Don't do this for production code though, as this, [under usual circumstances, abnormally terminates your program](https://en.cppreference.com/w/c/program/abort).

To ensure that number **7** (correctly managing memory) is true, I used [Valgrind](https://valgrind.org/docs/manual/tech-docs.html):
{% highlight bash %}
gcc -g -Wall -Wextra main.c graph.c queue.c -o graphs
valgrind -s --track-origins=yes --leak-check=full ./graphs
{% endhighlight %}

Thankfully, Valgrind's output indeed shows no errors or leaks:
```
HEAP SUMMARY:
    in use at exit: 0 bytes in 0 blocks
  total heap usage: 132 allocs, 132 frees, 10,720 bytes allocated

All heap blocks were freed -- no leaks are possible

ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from 0)
```

The program's standard output:
```
./graphs
Graph as adjacency list:
0: -> 8 (weight: 8.00) -> 1 (weight: 4.00) 
1: -> 3 (weight: 8.00) -> 8 (weight: 11.00) -> 0 (weight: 4.00) 
3: -> 6 (weight: 4.00) -> 4 (weight: 7.00) -> 9 (weight: 2.00) -> 1 (weight: 8.00) 
4: -> 5 (weight: 9.00) -> 6 (weight: 14.00) -> 3 (weight: 7.00) 
5: -> 6 (weight: 10.00) -> 4 (weight: 9.00) 
6: -> 7 (weight: 2.00) -> 5 (weight: 10.00) -> 4 (weight: 14.00) -> 3 (weight: 4.00) 
7: -> 6 (weight: 2.00) -> 9 (weight: 6.00) -> 8 (weight: 1.00) 
8: -> 7 (weight: 1.00) -> 9 (weight: 7.00) -> 1 (weight: 11.00) -> 0 (weight: 8.00) 
9: -> 7 (weight: 6.00) -> 8 (weight: 7.00) -> 3 (weight: 2.00) 
Graph as adjacency matrix:
        0     1     3     4     5     6     7     8     9
  0     0  4.00     0     0     0     0     0  8.00     0
  1  4.00     0  8.00     0     0     0     0 11.00     0
  3     0  8.00     0  7.00     0  4.00     0     0  2.00
  4     0     0  7.00     0  9.00 14.00     0     0     0
  5     0     0     0  9.00     0 10.00     0     0     0
  6     0     0  4.00 14.00 10.00     0  2.00     0     0
  7     0     0     0     0     0  2.00     0  1.00  6.00
  8  8.00 11.00     0     0     0     0  1.00     0  7.00
  9     0     0  2.00     0     0     0  6.00  7.00     0

Visiting 0
Visiting 8
Visiting 1
Visiting 7
Visiting 9
Visiting 3
Visiting 6
Visiting 4
Visiting 5
Spanning tree as adjacency list:
0: -> 8 (weight: 8.00) -> 1 (weight: 4.00) 
1: -> 0 (weight: 4.00) 
3: -> 4 (weight: 7.00) -> 6 (weight: 4.00) -> 9 (weight: 2.00) 
4: -> 5 (weight: 9.00) -> 3 (weight: 7.00) 
5: -> 4 (weight: 9.00) 
6: -> 3 (weight: 4.00) -> 7 (weight: 2.00) 
7: -> 6 (weight: 2.00) -> 8 (weight: 1.00) 
8: -> 0 (weight: 8.00) -> 7 (weight: 1.00) 
9: -> 3 (weight: 2.00) 

Spanning tree as adjacency matrix:
        0     1     3     4     5     6     7     8     9
  0     0  4.00     0     0     0     0     0  8.00     0
  1  4.00     0     0     0     0     0     0     0     0
  3     0     0     0  7.00     0  4.00     0     0  2.00
  4     0     0  7.00     0  9.00     0     0     0     0
  5     0     0     0  9.00     0     0     0     0     0
  6     0     0  4.00     0     0     0  2.00     0     0
  7     0     0     0     0     0  2.00     0  1.00     0
  8  8.00     0     0     0     0     0  1.00     0     0
  9     0     0  2.00     0     0     0     0     0     0
```

<h3 id="wrapping-up">
  <a href="#wrapping-up" style="text-decoration: none; color: blue;">Wrapping up</a>
</h3>
First of all, if you reached this far I could somehow have your attention for a few minutes, so thank you!

I ran `time` and `perf` several times and reviewed the code thoroughly and can tell that
this project [isn't perfect](https://youtube.com/embed/dvKeCcxD3rQ?start=14&end=29). There are lots of potential improvements such as minimizing
system/kernel time by better placing allocations and releases, using memory pools etc.
Refactoring the code for higher testability and [cohesion](https://en.wikipedia.org/wiki/Cohesion_(computer_science)) is an 
interesting challenge as well. Nonetheless, I must draw the line somewhere.

Originally, the purpose of this post was to reinforce my inkling of experience on graphs and disjoint sets,
as my memory can get [brittle over time](https://en.wikipedia.org/wiki/Forgetting_curve).
However, as I wrote the article, I realized it can be the commence of a fruitful, long-lasting
blogging journey (hopefully). I also managed to redefine the purpose of it: to **share knowledge with
other people and the world**. This matters a lot to me. So, if this reading
meant something for you, please [let me know](mailto:omarmsqt@gmail.com).

This wasn't supposed to teach you everything on graphs, disjoint sets, and Kruskal's.
Rather, I hope I made your **brain scratch a little with the stuff I narrated**.